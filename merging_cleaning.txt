*)Facebook 
 0)df = pd.read_csv('dataset_Facebook.csv',sep=';')
 1)create data subset = df[df['Type']=='Photo']
 2)df['Type'].unique() ==gives unique values
 3)df.isna().sum()
 4)df.dropna()
 5)df.isnull().sum()
 
 Merging two subsets
 1)df_merge = pd.merge(photo_sub,video_sub,how='outer')
 2)df.sort_values('Page Total Likes')
 3)df_transpose = df.transpose()

 Melting of data = 
  df_melt = pd.melt(df,id_vars=['Type','Comment'],value_vars=['like','share'])
  
 casting of data :
 pd.pivot_table(df_melt,values='value',columns='variable',index=df.index,fill_value=0)

*)Data cleaning
  1)df.isna().sum()
  2)df.isnull().sum()
  3)(df=='?').sum()
  4)(df<0).sum()
  5)df.replace('?',np.nan)
    df.fillna(0)
    df.dropna()
  6)num_cols = df.select_dtypes(include='number')
    num_cols[num_cols<0].sum()


*)Data transformation 
# df['Species'] = df['Species'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})

  
*) outlier detection
   7)df['chol'].dtype
   df['chol'] = df['chol'].astype(float)
   print("Highest cholestrol allowed",df.chol.mean()+3*df.chol.std())
   print("Lowest allowed chol",df.chol.mean()-3*df.chol.std())

   df = df[(df['chol']<512) & (df['chol']>40)]
   df['oldpeak'] = df['oldpeak'].apply(lambda x:round(x))

or

def remove_outliers_zscore(data, threshold=3):
    z_scores = np.abs((data - data.mean()) / data.std())  # Calculate Z-scores
    
    out = z_scores > 3
    
    data = data[~out.any(axis=1)]
    
    return data
    
filtered_data = remove_outliers_zscore(df)

*)naive bayes and logistic regression

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Split the data into training and testing sets
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Build a logistic regression model
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Make predictions on the test set using the logistic regression model
y_pred_logreg = logreg.predict(X_test)

# Calculate accuracy for logistic regression model
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
accuracy_logreg

# Build a Na誰ve Bayes model
naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)

# Make predictions on the test set using the Na誰ve Bayes model
y_pred_nb = naive_bayes.predict(X_test)

# Calculate accuracy for Na誰ve Bayes model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
accuracy_nb

# Compare the accuracies of the two models
print("Accuracy - Logistic Regression:", accuracy_logreg)
print("Accuracy - Na誰ve Bayes:", accuracy_nb)

*)Model building
   x = df.drop(['num'],axis=1)
   x = df.drop(['num],axis=1).values
   x = df[:,:13].values
   y = df['num'].values

  from sklearn.model_selection import train_test_split
  from sklearn.model_selection import train_test_split
  X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25)

  from sklearn.neighbors import KNeighborsClassifier
  m1 = KNeighboursClassifier(n_neighbours=7)
  m1.fit(x_train,y_train)
  m1.predict(x_test)

  df['gender'] = df['gender'].replace({'Male':0,'Female':1})
  print(df)

 from sklearn import metrics
 print("hello",metrics.accuracy_score(y_test,y_pred))

 from sklearn.naive_bayes import MultinomialNB
 multinomialnb = MultinomialNB()
 
 from sklearn.linear_model import LogisticRegression
 logreg = LogisticRegression()
 .fit .predict

 from sklearn.linear_model import LinearRegression
 linreg = LinearRegression()

  metrics.accuracy_score(y_test,y_pred)*100
  metrics.precision_score(y_test,y_pred)
  metrics.recall_score(y_test,y_train)
  metrics.f1_score(y_test,y_pred)